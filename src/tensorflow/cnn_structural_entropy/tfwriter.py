import argparse
import tensorflow as tf
import os
project_path = os.path.dirname(os.path.realpath("../../../"))
import sys
import csv
sys.path.append(project_path)
from pe_parser.hexadecimal_parser import HexParser
from pe_parser.utils import serialize_structural_entropy_example
import numpy as np
import pandas as pd



MAX_HEX_VALUES = 16000000


def microsoft_dataset_to_tfrecords(train_filepath,
                                   test_filepath,
                                   labels_filepath,
                                   tfrecords_filepath,
                                   chunk_size=256,
                                   log=257):

    training_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "training.tfrecords")
    validation_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "validation.tfrecords")
    test_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "test.tfrecords")

    df_labels = pd.read_csv(labels_filepath)
    df_labels = df_labels.sample(frac=1).reset_index(drop=True)
    print(df_labels)
    total_samples = len(df_labels)
    training_samples = int((total_samples / 10) * 9)
    print(len(df_labels), training_samples)
    # Generate training and validation splits
    train_labels = df_labels[:9781]
    validation_labels = df_labels[9781:]
    print(train_labels, len(train_labels))
    print(validation_labels, len(validation_labels))


    NUM_MAX_SUBSETS = int(MAX_HEX_VALUES/chunk_size)
    # Training TFRecord

    j = 0
    for i in train_labels.index:
        print("{};{}".format(j, train_labels.loc[i, "Id"], train_labels.loc[i, "Class"]))
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(train_filepath+train_labels.loc[i, "Id"]+".bytes")
        hex_values = hexParser.extract_hex_values()
        hex_values = hexParser.convert_hex_values_to_int(preprocess=True, hex_values=hex_values)
        structural_entropy = hexParser.extract_structural_entropy(hex_values, chunk_size=chunk_size, log=log)
        if len(structural_entropy) <= NUM_MAX_SUBSETS:
            structural_entropy = np.pad(structural_entropy, (0, NUM_MAX_SUBSETS - len(structural_entropy)),
                                        'constant',
                                        constant_values=(0, 0))
        else:
            structural_entropy = structural_entropy[:NUM_MAX_SUBSETS]
        serialized_example = serialize_structural_entropy_example(train_labels.loc[i, "Id"],
                                                                  structural_entropy,
                                                                  int(train_labels.loc[i, 'Class']) - 1)
        training_tfwriter.write(serialized_example)
        j += 1

    j = 0
    for i in validation_labels.index:
        print("{};{}".format(j, validation_labels.loc[i, "Id"], validation_labels.loc[i, "Class"]))
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(train_filepath + validation_labels.loc[i, "Id"] + ".bytes")
        hex_values = hexParser.extract_hex_values()
        hex_values = hexParser.convert_hex_values_to_int(preprocess=True, hex_values=hex_values)
        structural_entropy = hexParser.extract_structural_entropy(hex_values, chunk_size=chunk_size, log=log)
        if len(structural_entropy) <= NUM_MAX_SUBSETS:
            structural_entropy = np.pad(structural_entropy, (0, NUM_MAX_SUBSETS - len(structural_entropy)),
                                        'constant',
                                        constant_values=(0, 0))
        else:
            structural_entropy = structural_entropy[:NUM_MAX_SUBSETS]
        serialized_example = serialize_structural_entropy_example(validation_labels.loc[i, "Id"],
                                                                  structural_entropy,
                                                                  int(validation_labels.loc[i, 'Class']) - 1)
        validation_tfwriter.write(serialized_example)
        j += 1

    j = 0
    for filename in os.listdir(test_filepath):
        Id = filename[:-6]
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(test_filepath + filename)
        hex_values = hexParser.extract_hex_values()
        hex_values = hexParser.convert_hex_values_to_int(preprocess=True, hex_values=hex_values)
        structural_entropy = hexParser.extract_structural_entropy(hex_values, chunk_size=chunk_size, log=log)
        if len(structural_entropy) <= NUM_MAX_SUBSETS:
            structural_entropy = np.pad(structural_entropy, (0, NUM_MAX_SUBSETS - len(structural_entropy)),
                                        'constant',
                                        constant_values=(0, 0))
        else:
            structural_entropy = structural_entropy[:NUM_MAX_SUBSETS]
        serialized_example = serialize_structural_entropy_example(Id,
                                                                  structural_entropy,
                                                                  0)
        test_tfwriter.write(serialized_example)
        j += 1



trainLabels = "../../../data/feature_files/train/trainLabels.csv"
train_filepath = "/mnt/hdd1/cerberus_mlw_data/hexadecimal/train/"
test_filepath = "/mnt/hdd1/cerberus_mlw_data/hexadecimal/test/"

tfrecords_filepath = "/mnt/hdd1/cerberus_mlw_data/tfrecords/structural_entropy/256/"
microsoft_dataset_to_tfrecords(train_filepath, test_filepath, trainLabels, tfrecords_filepath, chunk_size=256)

tfrecords_filepath = "/mnt/hdd1/cerberus_mlw_data/tfrecords/structural_entropy/1024/"
microsoft_dataset_to_tfrecords(train_filepath, test_filepath, trainLabels, tfrecords_filepath, chunk_size=1024)

tfrecords_filepath = "/mnt/hdd1/cerberus_mlw_data/tfrecords/structural_entropy/4096/"
microsoft_dataset_to_tfrecords(train_filepath, test_filepath, trainLabels, tfrecords_filepath, chunk_size=4096)
