import argparse
import tensorflow as tf
import sys
import os
project_path = os.path.dirname(os.path.realpath("../../"))
sys.path.append(project_path)
from src.tensorflow.shallow_cnn_bytes.tfreader import make_dataset
from pe_parser.utils import load_vocabulary, load_parameters, create_lookup_table
from src.tensorflow.shallow_cnn_bytes.shallow_cnn import ShallowCNN
import collections
import numpy as np

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Shallow CNN Model Training')
    parser.add_argument("model",
                        type=str,
                        help="Tensorflow model")
    parser.add_argument("tfrecord",
                        type=str,
                        help="TFrecord containing the samples used to evaluate the model")
    parser.add_argument("parameters",
                        type=str,
                        help="JSON file containing the parameters of the model")
    parser.add_argument("vocabulary_mapping_filepath",
                        type=str,
                        help="Filepath describing the vocabulary mapping between mnemonics and IDs")
    parser.add_argument("output_filepath",
                        type=str,
                        help="Output filepath")
    args = parser.parse_args()

    tf.config.set_visible_devices([], 'GPU')

    vocabulary_mapping = load_vocabulary(args.vocabulary_mapping_filepath)
    lookup_table = create_lookup_table(vocabulary_mapping, 1)

    latest = tf.train.latest_checkpoint(args.model)
    parameters = load_parameters(args.parameters)
    model = ShallowCNN(parameters)
    model.load_weights(latest)

    loss_func = tf.keras.losses.SparseCategoricalCrossentropy()

    test_epoch_loss_avg = tf.keras.metrics.Mean()
    test_epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

    predictions = collections.OrderedDict()

    # Evaluate model on the test set
    d_test = make_dataset(args.tfrecord,
                          lookup_table,
                          1,
                          1,
                          1)
    columns = ['Id', "Prediction1", "Prediction2", "Prediction3", "Prediction4", "Prediction5", "Prediction6",
               "Prediction7", "Prediction8", "Prediction9"]

    with open(args.output_filepath, "w") as output_file:
        output_file.write(",".join(columns))
        output_file.write("\n")

        i = 0
        for id_batch_test, x_batch_test, y_batch_test in d_test:
            test_logits = model(x_batch_test, False)
            id = str(np.array(id_batch_test[0]))[2:-1]
            print(i, id)
            test_logits = np.array(test_logits).tolist()[0]
            test_logits = ["%2.6f"% x for x in test_logits]
            output_file.write("{},".format(id))
            output_file.write(",".join(test_logits))
            output_file.write("\n")
            i+=1

