import argparse
import tensorflow as tf
import os
project_path = os.path.dirname(os.path.realpath("../../"))
import sys
sys.path.append(project_path)
from src.tensorflow.shallow_cnn_bytes.shallow_cnn import ShallowCNN
from src.tensorflow.shallow_cnn_bytes.tfreader import make_dataset
from pe_parser.utils import load_vocabulary, load_parameters, create_lookup_table
import pandas as pd
from sklearn.metrics import confusion_matrix

tf.get_logger().setLevel('INFO')


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Shallow CNN Model Training')
    parser.add_argument("model",
                        type=str,
                        help="Model name")
    parser.add_argument("tr_tfrecord",
                        type=str,
                        help="Training TFRecord file")
    parser.add_argument("val_tfrecord",
                        type=str,
                        help="Validation TFrecord file")
    parser.add_argument("parameters",
                        type=str,
                        help="JSON file containing the parameters of the model")
    parser.add_argument("vocabulary_mapping_filepath",
                        type=str,
                        help="Filepath describing the vocabulary mapping between mnemonics and IDs")
    args = parser.parse_args()

    print("TensorFlow version: {}".format(tf.__version__))
    print("Eager execution: {}".format(tf.executing_eagerly()))
    print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
    tf.debugging.set_log_device_placement(False)

    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
            logical_gpus = tf.config.experimental.list_logical_devices('GPU')
            print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPU")
        except RuntimeError as e:
            # Visible devices must be set before GPUs have been initialized
            print(e)

    #Load vocabulary and create lookup table
    vocabulary_mapping = load_vocabulary(args.vocabulary_mapping_filepath)
    lookup_table = create_lookup_table(vocabulary_mapping, 1)

    # Load parameters of the model
    parameters = load_parameters(args.parameters)

    # Specify GPU
    if "gpu" in parameters.keys():
        os.environ["CUDA_VISIBLE_DEVICES"] = parameters["gpu"]


    model = ShallowCNN(parameters)
    if os.path.isdir("models/{}/".format(args.model)):
        print("LOADING WEIGHTS!!!!")
        latest = tf.train.latest_checkpoint("models/{}/".format(args.model))
        model.load_weights(latest)

    loss_func = tf.keras.losses.SparseCategoricalCrossentropy()
    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
    optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'])


    def train_loop(features, labels, training=False):
        # Define the GradientTape context
        with tf.GradientTape() as tape:
            # Get the probabilities
            predictions = model(features, training)
            #labels = tf.dtypes.cast(labels, tf.float32)
            # Calculate the loss
            loss = loss_func(labels, predictions)
        # Get the gradients
        gradients = tape.gradient(loss, model.trainable_variables)
        # Update the weights
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        return loss, predictions


    # Training loop
    # 1/ Iterate each epoch. An epoch is one pass through the dataset
    # 2/ Whithin an epoch, iterate over each example in the training Dataset.
    # 3/ Calculate model's loss and gradients
    # 4/ Use an optimizer to update the model's variables
    # 5/ Keep track of stats and repeat

    train_loss_results = []
    train_accuracy_results = []

    validation_loss_results = []
    validation_accuracy_results = []

    if not os.path.isdir("models/{}/".format(args.model)):
        os.mkdir("models/{}/".format(args.model))

    with open("models/{}/training_metrics.out".format(args.model), "w") as loss_file:
        loss_file.write("training_loss,validation_loss,training_accuracy,validation_accuracy\n")

        #checkpoint_path = "models/ShallowCNN/model_ep_{}.ckpt"
        #checkpoint_dir = os.path.dirname(checkpoint_path)

        num_epochs = parameters['epochs']

        initial_loss = 10.0
        for epoch in range(num_epochs):
            print("Current epoch: {}".format(epoch))
            checkpoint_path = "models/{}/model_001.ckpt".format(args.model)
            #checkpoint_dir = os.path.dirname(checkpoint_path)

            d_train = make_dataset(args.tr_tfrecord,
                                   lookup_table,
                                   parameters['buffer_size'],
                                   parameters['batch_size'],
                                   1)
            d_val = make_dataset(args.val_tfrecord,
                                 lookup_table,
                                 1024,
                                 1,
                                 1)


            # Training metrics
            epoch_loss_avg = tf.keras.metrics.Mean()
            epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
            # Validation metrics
            val_epoch_loss_avg = tf.keras.metrics.Mean()
            val_epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
            tr_step = 0

            # Training loop
            for step, (id, x, y) in enumerate(d_train):
                #print("Id: {}\n X={}\n x.shape: {}\ny={}\n".format(id,x,x.shape,y))
                #print("Input: {}".format(x))

                loss, y_ = train_loop(x, y, True)

                # Track progress
                epoch_loss_avg(loss)
                epoch_accuracy(y, y_)
                print("Iteration step: {}; Loss: {:.3f}, Accuracy: {:.3%}".format(tr_step,
                                                                                  epoch_loss_avg.result(),
                                                                                  epoch_accuracy.result()))
                tr_step += 1

            # End epoch
            train_loss_results.append(epoch_loss_avg.result())
            train_accuracy_results.append(epoch_accuracy.result())



            # Run a validation loop at the end of each epoch.
            for id_batch_val, x_batch_val, y_batch_val in d_val:
                val_logits = model(x_batch_val, False)
                val_loss = loss_func(y_batch_val, val_logits)

                # Update metrics
                val_epoch_loss_avg(val_loss)
                val_epoch_accuracy(y_batch_val, val_logits)

            loss_file.write(
                "{},{},{},{}\n".format(epoch_loss_avg.result(), val_epoch_loss_avg.result(), epoch_accuracy.result(),
                                       val_epoch_accuracy.result()))

            val_acc = val_epoch_accuracy.result()
            val_loss = val_epoch_loss_avg.result()
            print('Epoch: {}; Validation loss {}; acc: {}'.format(epoch, val_loss, val_acc))

            validation_loss_results.append(val_loss)
            validation_accuracy_results.append(val_acc)

            if float(val_loss) < initial_loss:
                initial_loss = float(val_loss)
                model.save_weights(checkpoint_path) # Save only the weights


