import argparse
import tensorflow as tf
import sys
import os
project_path = os.path.dirname(os.path.realpath("../../"))
sys.path.append(project_path)
from src.tensorflow.shallow_cnn_bytes.tfreader import make_dataset
from pe_parser.utils import load_vocabulary, load_parameters, create_lookup_table
from src.tensorflow.shallow_cnn_bytes.shallow_cnn import ShallowCNN
import numpy as np
import csv


training_tfrecord = "/mnt/hdd1/cerberus_mlw_data/tfrecords/bytes/training.tfrecords"
validation_tfrecord = "/mnt/hdd1/cerberus_mlw_data/tfrecords/bytes/validation.tfrecords"
testing_tfrecord = "/mnt/hdd1/cerberus_mlw_data/tfrecords/bytes/test.tfrecords"

training_output_filepath = "/home/kaito/postdoc_projects/malware_classification_with_gradient_boosting_and_deep_features/data/feature_files/train/subsets_with_labels/bytes_cnn_features.csv"
test_output_filepath = "/home/kaito/postdoc_projects/malware_classification_with_gradient_boosting_and_deep_features/data/feature_files/test/bytes_cnn_features.csv"


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Shallow CNN Model Training')
    parser.add_argument("model",
                        type=str,
                        help="Tensorflow model")
    parser.add_argument("parameters",
                        type=str,
                        help="JSON file containing the parameters of the model")
    parser.add_argument("vocabulary_mapping_filepath",
                        type=str,
                        help="Filepath describing the vocabulary mapping between mnemonics and IDs")
    parser.add_argument("num_features",
                        type=int,
                        help="Number of features after the global max-pooling layer")
    args = parser.parse_args()

    tf.config.set_visible_devices([], 'GPU')

    vocabulary_mapping = load_vocabulary(args.vocabulary_mapping_filepath)
    lookup_table = create_lookup_table(vocabulary_mapping, 1)

    latest = tf.train.latest_checkpoint(args.model)
    parameters = load_parameters(args.parameters)
    model = ShallowCNN(parameters)
    model.load_weights(latest)

    columns_without_class = ["Id"]
    for i in range(1, args.num_features + 1):
        columns_without_class.append("BYTE_CNN_Feature_{}".format(i))

    columns = ["Id"]
    for i in range(1,args.num_features+1):
        columns.append("BYTE_CNN_Feature_{}".format(i))
    columns.append("Class")

    #print(columns)
    d_train =  make_dataset(training_tfrecord,
                          lookup_table,
                          1,
                          1,
                          1)

    d_val = make_dataset(validation_tfrecord,
                           lookup_table,
                           1,
                           1,
                           1)

    i = 0
    with open(training_output_filepath, "w") as output_file:
        writer = csv.DictWriter(output_file, fieldnames=columns)
        writer.writeheader()

        # Training tfrecord
        for id_batch, x_batch, y_batch in d_train:
            id = str(np.array(id_batch[0]))[2:-1]
            family = np.array(y_batch)[0]
            print(i, id, family)
            predictions = model(x_batch, False)
            features = model.extract_features(x_batch)
            features = np.array(features)

            cnn_features = dict()
            cnn_features['Id'] = id
            cnn_features['Class'] = family

            j = 1
            for feature in features[0]:
                cnn_features["BYTE_CNN_Feature_{}".format(j)] = feature
                j += 1
            writer.writerow({feature_name: cnn_features[feature_name] for feature_name in columns})


            #print(features[0], features.shape)
            i += 1

        # Validation tfrecord
        for id_batch, x_batch, y_batch in d_val:
            id = str(np.array(id_batch[0]))[2:-1]
            family = np.array(y_batch)[0]
            print(i, id, family)
            predictions = model(x_batch, False)
            features = model.extract_features(x_batch)
            features = np.array(features)

            cnn_features = dict()
            cnn_features['Id'] = id
            cnn_features['Class'] = family

            j = 1
            for feature in features[0]:
                cnn_features["BYTE_CNN_Feature_{}".format(j)] = feature
                j += 1
            writer.writerow({feature_name: cnn_features[feature_name] for feature_name in columns})

            # print(features[0], features.shape)
            i += 1

    # Test tfrecord
    d_test = make_dataset(testing_tfrecord,
                         lookup_table,
                         1,
                         1,
                         1)
    i = 0
    with open(test_output_filepath, "w") as output_file:
        writer = csv.DictWriter(output_file, fieldnames=columns_without_class)
        writer.writeheader()

        for id_batch, x_batch, y_batch in d_test:
            id = str(np.array(id_batch[0]))[2:-1]
            print(i, id)
            predictions = model(x_batch, False)
            features = model.extract_features(x_batch)
            features = np.array(features)

            cnn_features = dict()
            cnn_features['Id'] = id

            j = 1
            for feature in features[0]:
                cnn_features["BYTE_CNN_Feature_{}".format(j)] = feature
                j += 1
            writer.writerow({feature_name: cnn_features[feature_name] for feature_name in columns_without_class})
            i += 1


