import argparse
import tensorflow as tf
import os
import sys
import csv
from pe_parser.hexadecimal_parser import HexParser
from pe_parser.utils import load_vocabulary
from pe_parser.utils import serialize_bytes_example
import pandas as pd



def microsoft_dataset_to_tfrecords(train_filepath,
                                   test_filepath,
                                   labels_filepath,
                                   tfrecords_filepath,
                                   vocabulary_mapping_filepath,
                                   max_bytes=2000000):

    training_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "training.tfrecords")
    validation_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "validation.tfrecords")
    test_tfwriter = tf.io.TFRecordWriter(tfrecords_filepath + "test.tfrecords")

    vocabulary_mapping = load_vocabulary(vocabulary_mapping_filepath)

    df_labels = pd.read_csv(labels_filepath)
    df_labels = df_labels.sample(frac=1).reset_index(drop=True)
    print(df_labels)
    total_samples = len(df_labels)
    training_samples= int((total_samples / 10) * 9)
    print(len(df_labels), training_samples)
    #Generate training and validation splits
    train_labels = df_labels[:9781]
    validation_labels = df_labels[9781:]
    print(train_labels, len(train_labels))
    print(validation_labels, len(validation_labels))


    j = 0
    for i in train_labels.index:
        print("{};{}".format(j, train_labels.loc[i, "Id"], train_labels.loc[i, "Class"]))
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(train_filepath+train_labels.loc[i, "Id"]+".bytes")
        hex_values = hexParser.extract_hex_values()
        for k in range(len(hex_values)):
            if hex_values[k] not in vocabulary_mapping.keys():
                hex_values[k] = "UNK"
        if len(hex_values) < max_bytes:
            while len(hex_values) < max_bytes:
                hex_values.append("PAD")
        else:
            hex_values = hex_values[:max_bytes]
        raw_bytes_sequence = " ".join(hex_values)

        example = serialize_bytes_example(train_labels.loc[i, "Id"],
                                          raw_bytes_sequence,
                                          int(train_labels.loc[i, "Class"]) - 1)
        training_tfwriter.write(example)
        j += 1

    j = 0
    for i in validation_labels.index:
        print("{};{}".format(j, validation_labels.loc[i, "Id"], validation_labels.loc[i, "Class"]))
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(train_filepath + validation_labels.loc[i, "Id"] + ".bytes")
        hex_values = hexParser.extract_hex_values()
        for k in range(len(hex_values)):
            if hex_values[k] not in vocabulary_mapping.keys():
                hex_values[k] = "UNK"
        if len(hex_values) < max_bytes:
            while len(hex_values) < max_bytes:
                hex_values.append("PAD")
        else:
            hex_values = hex_values[:max_bytes]
        raw_bytes_sequence = " ".join(hex_values)

        example = serialize_bytes_example(validation_labels.loc[i, "Id"],
                                          raw_bytes_sequence,
                                          int(validation_labels.loc[i, "Class"]) - 1)
        validation_tfwriter.write(example)
        j += 1

    # Test set
    j = 0
    for filename in os.listdir(test_filepath):
        Id = filename[:-6]
        hexParser = HexParser()
        hexParser.load_hexadecimal_file(test_filepath + filename)
        hex_values = hexParser.extract_hex_values()
        for k in range(len(hex_values)):
            if hex_values[k] not in vocabulary_mapping.keys():
                hex_values[k] = "UNK"
        if len(hex_values) < max_bytes:
            while len(hex_values) < max_bytes:
                hex_values.append("PAD")
        else:
            hex_values = hex_values[:max_bytes]
        raw_bytes_sequence = " ".join(hex_values)

        example = serialize_bytes_example(Id, raw_bytes_sequence, 0)
        test_tfwriter.write(example)
        j += 1


trainLabels = "../../../data/feature_files/train/trainLabels.csv"
train_filepath = "/mnt/hdd1/cerberus_mlw_data/hexadecimal/train/"
test_filepath = "/mnt/hdd1/cerberus_mlw_data/hexadecimal/test/"
tfrecords_filepath = "/mnt/hdd1/cerberus_mlw_data/tfrecords/bytes/"
vocabulary_mapping_filepath="vocabulary/vocabulary_mapping.json"

microsoft_dataset_to_tfrecords(train_filepath, test_filepath, trainLabels, tfrecords_filepath, vocabulary_mapping_filepath, max_bytes=1000000)

