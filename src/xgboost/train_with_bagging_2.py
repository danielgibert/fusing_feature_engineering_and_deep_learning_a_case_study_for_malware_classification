import pandas as pd
import argparse
import json
import sys
sys.path.append("../../")
from src.xgboost.utils import split_dataframe_into_train_and_validation, argmax
import xgboost as xgb
import collections
import numpy as np
import csv

def load_parameters(hyperparameters_filepath):
    with open(hyperparameters_filepath, "r") as hyperparameters_file:
        hyperparameters = json.load(hyperparameters_file)
        return hyperparameters

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("training_filepath", help="CSV containing the features of the training samples", type=str)
    parser.add_argument("test_filepath", help="CSV containing the features of the test samples", type=str)
    parser.add_argument("hyperparameters_filepath", help="Hyperparameters filepath", type=str)
    parser.add_argument("output_filepath", help="output_filepath", type=str)
    parser.add_argument("K", help="Number of bagging splits", type=int)
    parser.add_argument("--voting", help="Voting strategy", type=str, default="None") #soft, hard, average
    parser.add_argument("--train_percentage",
                        help="Percentage of samples per split",
                        type=float,
                        default=0.9)
    parser.add_argument("--bagging_percentage",
                        help="Percentage of samples per split",
                        type=float,
                        default=2.0)
    args = parser.parse_args()

    hyperparameters = load_parameters(args.hyperparameters_filepath)
    num_datasets = args.K

    data = pd.read_csv(args.training_filepath)

    dict_of_test_predictions = collections.OrderedDict()
    list_of_dataframes = []
    list_of_xgboost_models = []
    list_of_outputs = []
    # Test
    df_test = pd.read_csv(args.test_filepath)
    test_ids = df_test['Id'].values
    df_test = df_test.drop(columns=['Id'])
    dtest = xgb.DMatrix(df_test)

    for i in range(num_datasets):
        df_train, df_val = split_dataframe_into_train_and_validation(data, prob=args.train_percentage)
        num_samples_per_split = int(df_train.shape[0]*args.bagging_percentage)
        df_train = df_train.sample(num_samples_per_split)


        # Create DMatrices
        train_labels = df_train["Class"]
        train_features = df_train.drop(columns=["Class", "Id"])
        #print(train_features.size, train_labels.size)

        dtrain = xgb.DMatrix(train_features, label=train_labels)

        val_labels = df_val["Class"]
        val_features = df_val.drop(columns=["Class", "Id"])
        #print(val_features.size, val_labels.size)
        dval = xgb.DMatrix(val_features, label=val_labels)

        evallist = [(dval, 'eval')]
        bst = xgb.train(hyperparameters, dtrain, hyperparameters["num_rounds"], evallist, early_stopping_rounds=10)
        list_of_xgboost_models.append(bst)

        output_test = bst.predict(dtest)
        list_of_outputs.append(output_test)

    columns = ["Id", "Prediction1", "Prediction2", "Prediction3", "Prediction4",
               "Prediction5", "Prediction6", "Prediction7", "Prediction8", "Prediction9"]
    with open(
            args.output_filepath + "_K={}_tr_per={}_bag_per={}_eta_{}_mcw_{}_gamma_{}_subsample_{}_colsample_bytree_{}_maxdepth_{}_voting_{}.csv".format(
                    args.K,
                    args.train_percentage,
                args.bagging_percentage,
                    hyperparameters["eta"],
                    hyperparameters["min_child_weight"],
                    hyperparameters["gamma"],
                    hyperparameters["subsample"],
                    hyperparameters["colsample_bytree"],
                    hyperparameters["max_depth"],
                    args.voting), "w") as output_file:
        writer = csv.DictWriter(output_file, fieldnames=columns)
        writer.writeheader()
        if args.voting == "hard":
            for i in range(len(test_ids)):
                predictions = np.zeros(hyperparameters["num_class"])
                for j in range(len(list_of_outputs)):
                    single_prediction = np.array(list_of_outputs[j][i])
                    vote_index =  np.unravel_index(argmax(single_prediction), single_prediction.shape)
                    predictions[vote_index] += 1

                majority_vote = np.unravel_index(argmax(predictions),
                                                 predictions.shape)  # Get majority vote
                predictions = np.zeros(hyperparameters["num_class"], dtype=np.float32)
                predictions[majority_vote] += 1.0

                writer.writerow({"Id": test_ids[i],
                                 "Prediction1": predictions[0],
                                 "Prediction2": predictions[1],
                                 "Prediction3": predictions[2],
                                 "Prediction4": predictions[3],
                                 "Prediction5": predictions[4],
                                 "Prediction6": predictions[5],
                                 "Prediction7": predictions[6],
                                 "Prediction8": predictions[7],
                                 "Prediction9": predictions[8]})

        elif args.voting == "soft":
            for i in range(len(test_ids)):
                predictions = np.zeros(hyperparameters["num_class"])
                for j in range(len(list_of_outputs)):
                    single_prediction = np.array(list_of_outputs[j][i])
                    predictions = np.sum([predictions, single_prediction], axis=0)
                majority_vote = np.unravel_index(argmax(predictions),
                                                 predictions.shape)  # Get majority vote
                predictions = np.zeros(hyperparameters["num_class"], dtype=np.float32)
                predictions[majority_vote] += 1.0

                writer.writerow({"Id": test_ids[i],
                                 "Prediction1": predictions[0],
                                 "Prediction2": predictions[1],
                                 "Prediction3": predictions[2],
                                 "Prediction4": predictions[3],
                                 "Prediction5": predictions[4],
                                 "Prediction6": predictions[5],
                                 "Prediction7": predictions[6],
                                 "Prediction8": predictions[7],
                                 "Prediction9": predictions[8]})
        elif args.voting == "average":

            for i in range(len(test_ids)):
                prediction1 = 0.0
                prediction2 = 0.0
                prediction3 = 0.0
                prediction4 = 0.0
                prediction5 = 0.0
                prediction6 = 0.0
                prediction7 = 0.0
                prediction8 = 0.0
                prediction9 = 0.0
                for j in range(len(list_of_outputs)):
                    prediction1 += list_of_outputs[j][i][0]
                    prediction2 += list_of_outputs[j][i][1]
                    prediction3 += list_of_outputs[j][i][2]
                    prediction4 += list_of_outputs[j][i][3]
                    prediction5 += list_of_outputs[j][i][4]
                    prediction6 += list_of_outputs[j][i][5]
                    prediction7 += list_of_outputs[j][i][6]
                    prediction8 += list_of_outputs[j][i][7]
                    prediction9 += list_of_outputs[j][i][8]


                writer.writerow({"Id": test_ids[i],
                                 "Prediction1": prediction1 / num_datasets,
                                 "Prediction2": prediction2 / num_datasets,
                                 "Prediction3": prediction3 / num_datasets,
                                 "Prediction4": prediction4 / num_datasets,
                                 "Prediction5": prediction5 / num_datasets,
                                 "Prediction6": prediction6 / num_datasets,
                                 "Prediction7": prediction7 / num_datasets,
                                 "Prediction8": prediction8 / num_datasets,
                                 "Prediction9": prediction9 / num_datasets})





