import xgboost as xgb
import pandas as pd
import argparse
import sys
sys.path.append("../../")
from src.xgboost.utils import split_dataframe_into_train_and_validation
from scipy.special import softmax
import csv
import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns


def load_parameters(hyperparameters_filepath):
    with open(hyperparameters_filepath, "r") as hyperparameters_file:
        hyperparameters = json.load(hyperparameters_file)
        return hyperparameters

def show_values_on_bars(axs, h_v="v", space=0.4):
    def _show_on_single_plot(ax):
        if h_v == "v":
            for p in ax.patches:
                _x = p.get_x() + p.get_width() / 2
                _y = p.get_y() + p.get_height()
                value = int(p.get_height())
                ax.text(_x, _y, value, ha="center")
        elif h_v == "h":
            for p in ax.patches:
                _x = p.get_x() + p.get_width() + float(space)
                _y = p.get_y() + p.get_height()
                value = int(p.get_width())
                ax.text(_x, _y, value, ha="left")

    if isinstance(axs, np.ndarray):
        for idx, ax in np.ndenumerate(axs):
            _show_on_single_plot(ax)
    else:
        _show_on_single_plot(axs)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("training_filepath", help="CSV containing the features of the training samples", type=str)
    parser.add_argument("hyperparameters_filepath", help="Hyperparameters filepath", type=str)
    parser.add_argument("features_importance_filepath", help="Feature importance output", type=str)
    parser.add_argument("--prob_split", help="Prob split", type=float, default=0.9)

    args = parser.parse_args()

    df = pd.read_csv(args.training_filepath)
    df['Class'] = df['Class']
    labels_range = set(df['Class'].values)
    #print("Labels range: {}".format(labels_range))
    #print(len(df.index))
    df_train, df_val = split_dataframe_into_train_and_validation(df,args.prob_split)
    #print(len(df_train.index), len(df_val.index))

    train_labels = df_train["Class"]
    train_features = df_train.drop(columns=["Class", "Id"])
    dtrain = xgb.DMatrix(train_features, label=train_labels)

    val_labels = df_val["Class"]
    val_features = df_val.drop(columns=["Class", "Id"])
    dval = xgb.DMatrix(val_features, label=val_labels)

    hyperparameters = load_parameters(args.hyperparameters_filepath)

    evallist = [(dval, 'eval')]

    bst = xgb.train(hyperparameters, dtrain, hyperparameters["num_rounds"], evallist, early_stopping_rounds=10)
    #bst.dump_model('{}_dump_raw.txt'.format(args.model_filepath), '{}_featmap.txt'.format(args.model_filepath))

    weight_scores = bst.get_score(importance_type='weight')
    keys = list(weight_scores.keys())
    values = list(weight_scores.values())
    data = pd.DataFrame()
    data["Feature_name"] = keys
    data["Value"] = values
    data.to_csv(args.features_importance_filepath+"weight.out", index=False)

    gain_scores = bst.get_score(importance_type='gain')
    keys = list(gain_scores.keys())
    values = list(gain_scores.values())
    data = pd.DataFrame()
    data["Feature_name"] = keys
    data["Value"] = values
    data.to_csv(args.features_importance_filepath+"gain.out", index=False)


    cover_scores = bst.get_score(importance_type='cover')
    keys = list(cover_scores.keys())
    values = list(cover_scores.values())
    data = pd.DataFrame()
    data["Feature_name"] = keys
    data["Value"] = values
    data.to_csv(args.features_importance_filepath+"cover.out", index=False)

