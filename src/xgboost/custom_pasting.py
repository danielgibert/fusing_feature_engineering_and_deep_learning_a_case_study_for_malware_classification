import pandas as pd
import xgboost as xgb
from utils import split_dataframe_into_train_and_validation, argmax
import collections
import json
import numpy as np
import csv
import argparse


def load_parameters(hyperparameters_filepath):
    with open(hyperparameters_filepath, "r") as hyperparameters_file:
        hyperparameters = json.load(hyperparameters_file)
        return hyperparameters

class CustomBagging():
    def __init__(self, hyperparameters_filepath, output_filepath):
        self.hyperparameters = load_parameters(hyperparameters_filepath)
        self.output_filepath = output_filepath

        self.train_filepath_index = 0
        self.test_filepath_index = 1
        self.model_filepath_index = 2
        self.num_models_index = 3
        self.model_name = 4

        self.train_test_individual_model_tuples = [
            ["../../data/feature_files/train/subsets_with_labels/asm_api_features.csv",
             "../../data/feature_files/test/subsets/asm_api_features.csv",
             "models/custom_bagging/asm_api_features_model_", 1, "API"],
            ["../../data/feature_files/train/subsets_with_labels/asm_cnn_opcode_features.csv",
             "../../data/feature_files/test/subsets/asm_cnn_opcode_features.csv",
             "models/custom_bagging/asm_cnn_opcode_features_model_", 1, "CNN_OPC"],
            ["../../data/feature_files/train/subsets_with_labels/asm_data_define_features.csv",
             "../../data/feature_files/test/subsets/asm_data_define_features.csv",
             "models/custom_bagging/asm_data_define_features_", 1, "DD"],
            ["../../data/feature_files/train/subsets_with_labels/asm_misc_features.csv",
             "../../data/feature_files/test/subsets/asm_misc_features.csv",
             "models/custom_bagging/asm_misc_features_model_", 1, "MISC"],
            ["../../data/feature_files/train/subsets_with_labels/asm_opcode_features.csv",
             "../../data/feature_files/test/subsets/asm_opcode_features.csv",
             "models/custom_bagging/asm_opcode_features_model", 1, "OPC"],
            ["../../data/feature_files/train/subsets_with_labels/asm_register_features.csv",
             "../../data/feature_files/test/subsets/asm_register_features.csv",
             "models/custom_bagging/asm_register_features_model_", 1, "REG"],
            ["../../data/feature_files/train/subsets_with_labels/asm_section_features.csv",
             "../../data/feature_files/test/subsets/asm_section_features.csv",
             "models/custom_bagging/asm_section_features_model_", 1, "SEC"],
            ["../../data/feature_files/train/subsets_with_labels/asm_symbol_features.csv",
             "../../data/feature_files/test/subsets/asm_symbol_features.csv",
             "models/custom_bagging/asm_symbol_features_model_", 1, "SYM"],
            ["../../data/feature_files/train/subsets_with_labels/byte_cnn_entropy_features.csv",
             "../../data/feature_files/test/subsets/byte_cnn_entropy_features.csv",
             "models/custom_bagging/byte_cnn_entropy_features_model_", 1, "CNN_ENT"],
            ["../../data/feature_files/train/subsets_with_labels/byte_cnn_img_features.csv",
             "../../data/feature_files/test/subsets/byte_cnn_img_features.csv",
             "models/custom_bagging/byte_cnn_img_features_model_", 1, "CNN_IMG"],
            ["../../data/feature_files/train/subsets_with_labels/byte_entropy_features.csv",
             "../../data/feature_files/test/subsets/byte_entropy_features.csv",
             "models/custom_bagging/byte_entropy_features_model_", 1, "ENT"],
            ["../../data/feature_files/train/subsets_with_labels/byte_img_haralick_features.csv",
             "../../data/feature_files/test/subsets/byte_img_haralick_features.csv",
             "models/custom_bagging/byte_img_haralick_features_model_", 1, "HAR"],
            ["../../data/feature_files/train/subsets_with_labels/byte_img_lbp_features.csv",
             "../../data/feature_files/test/subsets/byte_img_lbp_features.csv",
             "models/custom_bagging/byte_img_lbp_features_model_", 1, "LBP"],
            ["../../data/feature_files/train/subsets_with_labels/byte_unigram_features.csv",
             "../../data/feature_files/test/subsets/byte_unigram_features.csv",
             "models/custom_bagging/byte_unigram_features_model_", 1, "B"],
            ["../../data/feature_files/train/subsets_with_labels/bytes_cnn_features.csv",
             "../../data/feature_files/test/subsets/bytes_cnn_features.csv",
             "models/custom_bagging/bytes_cnn_features_model_", 1, "BC"],
            ["../../data/feature_files/train/feature_concatenation/asm_view_features.csv",
             "../../data/feature_files/test/feature_concatenation/asm_view_features.csv",
             "models/custom_bagging/asm_view_features_model_", 4, "AV"],
            ["../../data/feature_files/train/feature_concatenation/asm_view_hand_crafted_features.csv",
             "../../data/feature_files/test/feature_concatenation/asm_view_hand_crafted_features.csv",
             "models/custom_bagging/asm_view_hand_crafted_features_model_", 1, "AVHC"],
            ["../../data/feature_files/train/feature_concatenation/byte_view_features.csv",
             "../../data/feature_files/test/feature_concatenation/byte_view_features.csv",
             "models/custom_bagging/byte_view_features_model_", 1, "BV"],
            ["../../data/feature_files/train/feature_concatenation/byte_view_hand_crafted_features.csv",
             "../../data/feature_files/test/feature_concatenation/byte_view_hand_crafted_features.csv",
             "models/custom_bagging/byte_view_hand_crafted_features_model_", 1, "BVHC"],
            ["../../data/feature_files/train/feature_concatenation/deep_features.csv",
             "../../data/feature_files/test/feature_concatenation/deep_features.csv",
             "models/custom_bagging/deep_features_model_", 1, "DP"],
            ["../../data/feature_files/train/feature_concatenation/hand_crafted_features.csv",
             "../../data/feature_files/test/feature_concatenation/hand_crafted_features.csv",
             "models/custom_bagging/hand_crafted_features_model_", 1, "HC"],
            ["../../data/feature_files/train/feature_concatenation/all_features.csv",
             "../../data/feature_files/test/feature_concatenation/all_features.csv",
             "models/custom_bagging/all_features_model_", 10, "ALL"],
        ]

        for i in range(len(self.train_test_individual_model_tuples)):
            self.output_filepath = self.output_filepath +"_{}".format(self.train_test_individual_model_tuples[i][self.num_models_index])


    def fit_and_predict(self, train_percentage=0.9, voting=None):
        dict_of_test_predictions = collections.OrderedDict()
        list_of_dataframes = []
        list_of_xgboost_models = []
        list_of_outputs = []



        for train_test_model_tuple in self.train_test_individual_model_tuples:
            data = pd.read_csv(train_test_model_tuple[self.train_filepath_index])

            df_test = pd.read_csv(train_test_model_tuple[self.test_filepath_index])
            test_ids = df_test['Id'].values
            df_test = df_test.drop(columns=['Id'])
            dtest = xgb.DMatrix(df_test)

            for i in range(train_test_model_tuple[self.num_models_index]):
                df_train, df_val = split_dataframe_into_train_and_validation(data, train_percentage)

                # Create DMatrices
                train_labels = df_train["Class"]
                train_features = df_train.drop(columns=["Class", "Id"])
                dtrain = xgb.DMatrix(train_features, label=train_labels)

                val_labels = df_val["Class"]
                val_features = df_val.drop(columns=["Class", "Id"])
                dval = xgb.DMatrix(val_features, label=val_labels)

                evallist = [(dval, 'eval')]
                bst = xgb.train(self.hyperparameters, dtrain, self.hyperparameters["num_rounds"], evallist,
                                early_stopping_rounds=10)
                list_of_xgboost_models.append(bst)

                output_test = bst.predict(dtest)
                list_of_outputs.append(output_test)
        columns = ["Id", "Prediction1", "Prediction2", "Prediction3", "Prediction4",
                   "Prediction5", "Prediction6", "Prediction7", "Prediction8", "Prediction9"]
        with open(
                self.output_filepath + "_perr={}_eta_{}_mcw_{}_g_{}_sample_{}_col_{}_d_{}_v_{}.csv".format(
                    train_percentage,
                    self.hyperparameters["eta"],
                    self.hyperparameters["min_child_weight"],
                    self.hyperparameters["gamma"],
                    self.hyperparameters["subsample"],
                    self.hyperparameters["colsample_bytree"],
                    self.hyperparameters["max_depth"],
                    voting), "w") as output_file:
            writer = csv.DictWriter(output_file, fieldnames=columns)
            writer.writeheader()
            if voting == "hard":
                for i in range(len(test_ids)):
                    predictions = np.zeros(self.hyperparameters["num_class"])
                    for j in range(len(list_of_outputs)):
                        single_prediction = np.array(list_of_outputs[j][i])
                        vote_index = np.unravel_index(argmax(single_prediction), single_prediction.shape)
                        predictions[vote_index] += 1

                    majority_vote = np.unravel_index(argmax(predictions),
                                                     predictions.shape)  # Get majority vote
                    predictions = np.zeros(self.hyperparameters["num_class"], dtype=np.float32)
                    predictions[majority_vote] += 1.0

                    writer.writerow({"Id": test_ids[i],
                                     "Prediction1": predictions[0],
                                     "Prediction2": predictions[1],
                                     "Prediction3": predictions[2],
                                     "Prediction4": predictions[3],
                                     "Prediction5": predictions[4],
                                     "Prediction6": predictions[5],
                                     "Prediction7": predictions[6],
                                     "Prediction8": predictions[7],
                                     "Prediction9": predictions[8]})

            elif voting == "soft":
                for i in range(len(test_ids)):
                    predictions = np.zeros(self.hyperparameters["num_class"])
                    for j in range(len(list_of_outputs)):
                        single_prediction = np.array(list_of_outputs[j][i])
                        predictions = np.sum([predictions, single_prediction], axis=0)
                    majority_vote = np.unravel_index(argmax(predictions),
                                                     predictions.shape)  # Get majority vote
                    predictions = np.zeros(self.hyperparameters["num_class"], dtype=np.float32)
                    predictions[majority_vote] += 1.0

                    writer.writerow({"Id": test_ids[i],
                                     "Prediction1": predictions[0],
                                     "Prediction2": predictions[1],
                                     "Prediction3": predictions[2],
                                     "Prediction4": predictions[3],
                                     "Prediction5": predictions[4],
                                     "Prediction6": predictions[5],
                                     "Prediction7": predictions[6],
                                     "Prediction8": predictions[7],
                                     "Prediction9": predictions[8]})
            elif voting == "average":
                num_datasets = int(sum([x[self.num_models_index] for x in self.train_test_individual_model_tuples]))
                for i in range(len(test_ids)):
                    prediction1 = 0.0
                    prediction2 = 0.0
                    prediction3 = 0.0
                    prediction4 = 0.0
                    prediction5 = 0.0
                    prediction6 = 0.0
                    prediction7 = 0.0
                    prediction8 = 0.0
                    prediction9 = 0.0
                    for j in range(len(list_of_outputs)):
                        prediction1 += list_of_outputs[j][i][0]
                        prediction2 += list_of_outputs[j][i][1]
                        prediction3 += list_of_outputs[j][i][2]
                        prediction4 += list_of_outputs[j][i][3]
                        prediction5 += list_of_outputs[j][i][4]
                        prediction6 += list_of_outputs[j][i][5]
                        prediction7 += list_of_outputs[j][i][6]
                        prediction8 += list_of_outputs[j][i][7]
                        prediction9 += list_of_outputs[j][i][8]

                    writer.writerow({"Id": test_ids[i],
                                     "Prediction1": prediction1 / num_datasets,
                                     "Prediction2": prediction2 / num_datasets,
                                     "Prediction3": prediction3 / num_datasets,
                                     "Prediction4": prediction4 / num_datasets,
                                     "Prediction5": prediction5 / num_datasets,
                                     "Prediction6": prediction6 / num_datasets,
                                     "Prediction7": prediction7 / num_datasets,
                                     "Prediction8": prediction8 / num_datasets,
                                     "Prediction9": prediction9 / num_datasets})


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("hyperparameters_filepath", help="Hyperparameters filepath", type=str)
    parser.add_argument("output_filepath", help="output_filepath", type=str)
    parser.add_argument("--voting", help="Voting strategy", type=str, default="None") #soft, hard
    parser.add_argument("--percentage",
                        help="Percentage of samples per split",
                        type=float,
                        default=0.9)
    args = parser.parse_args()
    custom_bagging = CustomBagging(args.hyperparameters_filepath, args.output_filepath)
    custom_bagging.fit_and_predict(args.percentage, args.voting)
