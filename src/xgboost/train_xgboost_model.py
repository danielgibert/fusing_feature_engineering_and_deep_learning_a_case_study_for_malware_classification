import xgboost as xgb
import pandas as pd
import argparse
import sys
sys.path.append("../../")
from src.xgboost.utils import split_dataframe_into_train_and_validation

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("training_filepath", help="CSV containing the features of the training samples", type=str)

    args = parser.parse_args()

    # Create Dataframes
    df = pd.read_csv(args.training_filepath)
    df['Class'] = df['Class'] -1 # Substract 1 to the 'Class' label
    labels_range = set(df['Class'].values)
    print("Labels range: {}".format(labels_range))
    print(df.size)
    df_train, df_val = split_dataframe_into_train_and_validation(df)

    # Create DMatrices
    train_labels = df_train["Class"]
    train_features = df_train.drop(columns=["Class", "Id"])
    print(train_features.size, train_labels.size)

    dtrain = xgb.DMatrix(train_features, label=train_labels)

    val_labels = df_val["Class"]
    val_features = df_val.drop(columns=["Class", "Id"])
    print(val_features.size, val_labels.size)
    dval = xgb.DMatrix(val_features, label=val_labels)

    # Hyperparameters
    param = {'max_depth': 3, 'eta': 1, 'objective': 'multi:softprob', 'num_class': 9}
    param['nthread'] = 4
    param['eval_metric'] = ['mlogloss'] # ['mlogloss', 'merror']
    evallist = [(dval, 'eval'), (dtrain, 'train')]

    # Training
    num_round = 200
    bst = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)
    bst.dump_model('models/dump.raw.txt', 'models/featmap.txt')
    bst.save_model('models/0001.model')
