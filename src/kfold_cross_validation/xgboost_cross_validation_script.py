import xgboost as xgb
from xgboost import cv
import json
import argparse
import pandas as pd
from xgboost.sklearn import XGBClassifier

def store_results_to_json(history, output_filepath):
    with open(output_filepath, "w") as output_file:
        json.dump(history, output_file)

def load_hyperparameters(hyperparameters_filepath):
    with open(hyperparameters_filepath, "r") as hyperparameters_file:
        hyperparameters = json.load(hyperparameters_file)
        return hyperparameters

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("training_filepath", help="CSV containing the features of the training samples", type=str)
    parser.add_argument("hyperparameters_filepath", help="Hyperparameters filepath", type=str)
    parser.add_argument("output_filepath", help="output_filepath", type=str)
    parser.add_argument("K", help="Number of folds", type=int)
    parser.add_argument('--substract', dest='remove', action='store_true')
    parser.add_argument('--no-substract', dest='remove', action='store_false')
    parser.set_defaults(remove=True)
    args = parser.parse_args()

    hyperparameters = load_hyperparameters(args.hyperparameters_filepath)

    xgb_model = XGBClassifier(hyperparameters, objective="multi:softprob")


    data = pd.read_csv(args.training_filepath)
    if args.remove is True:
        data['Class'] = data['Class'] - 1
    y = data['Class']
    X = data.drop(['Class', 'Id'], axis=1)
    dtrain = xgb.DMatrix(data=X, label=y)
    history = cv(hyperparameters, dtrain, num_boost_round=500, nfold=args.K, stratified=True,
                 metrics=["merror", "mlogloss"], early_stopping_rounds=5)
    history.to_csv(args.output_filepath)



